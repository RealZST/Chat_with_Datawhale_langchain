from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain.vectorstores import Chroma
import sys
sys.path.append("../")
from qa_chain.model_to_llm import model_to_llm
from qa_chain.get_vectordb import get_vectordb
import re


class QA_chain_self():
    """
    QA chain without conversation history.

    Loads the vector database, initializes the LLM instance, creates a prompt template, defines a retriever, 
    and constructs the QA chain.
    
    Answer generation process:
        - The retriever embeds the query and retrieves the top_k most relevant documents from the vector database.
        - The retrieved documents (context) are combined with the question and used to fill the QA_CHAIN_PROMPT.
        - The combined input is then passed to the LLM to generate the answer.
    """

    # Default prompt template that combines retrieval results with the query
    default_template_rq = """使用以下上下文来回答最后的问题。如果你不知道答案，就说你不知道，不要试图编造答
    案。最多使用三句话。尽量使答案简明扼要。总是在回答的最后说“谢谢你的提问！”。
    {context}
    问题: {question}
    有用的回答:"""

    def __init__(self, model:str, temperature:float=0.0, top_k:int=4, file_path:str=None, persist_path:str=None, appid:str=None, api_key:str=None, Spark_api_secret:str=None,Wenxin_secret_key:str=None, embedding = "m3e",  embedding_key = None, template=default_template_rq):
        self.model = model
        self.temperature = temperature
        self.top_k = top_k  # Number of top similar documents to retrieve
        self.file_path = file_path  # Path to the knowledge base files
        self.persist_path = persist_path  # Path to store the vector database
        self.appid = appid  # API parameter for Spark
        self.api_key = api_key
        self.Spark_api_secret = Spark_api_secret  # API secret for Spark
        self.Wenxin_secret_key = Wenxin_secret_key  # API secret for Wenxin
        self.embedding = embedding  # Selected embedding model
        self.embedding_key = embedding_key
        self.template = template  # prompt template
        
        # Load or create the vector database
        self.vectordb = get_vectordb(self.file_path, self.persist_path, self.embedding, self.embedding_key)
        
        # LLM instance
        self.llm = model_to_llm(self.model, self.temperature, self.appid, self.api_key, self.Spark_api_secret, self.Wenxin_secret_key)

        # Wrap the template with PromptTemplate for use in LangChain
        self.QA_CHAIN_PROMPT = PromptTemplate(input_variables=["context","question"], template=self.template)

        # Create a retriever from the vector database
        self.retriever = self.vectordb.as_retriever(search_type="similarity",   
                                        search_kwargs={'k': self.top_k})  #默认similarity，k=4
        # Define the QA chain
        self.qa_chain = RetrievalQA.from_chain_type(llm=self.llm,
                                        retriever=self.retriever,
                                        return_source_documents=True,
                                        chain_type_kwargs={"prompt":self.QA_CHAIN_PROMPT})
           
    def answer(self, question:str=None, temperature = None, top_k = 4):
        """
        Calls the QA chain to return the final answer.
        
        Args:
            question: User's question
        
        Return: 
            The answer generated by the LLM.
        """

        if len(question) == 0:
            return ""
        
        if temperature == None:
            temperature = self.temperature
            
        if top_k == None:
            top_k = self.top_k

        result = self.qa_chain({"query": question, "temperature": temperature, "top_k": top_k})
        answer = result["result"]
        answer = re.sub(r"\\n", '<br/>', answer)  # Replace \n with <br/> for formatting
        
        return answer   

